{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import textstat\n",
    "from language_tool_python import LanguageTool\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\habee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\habee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\habee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\habee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\habee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK RESOURCES\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, cleaned_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('input.csv')\n",
    "df.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic NLP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'] = df['cleaned_text'].apply(len) # char count\n",
    "\n",
    "df['word_count'] = df['cleaned_text'].apply(lambda x: len(word_tokenize(x))) # word count\n",
    "\n",
    "df['word_density'] = df['word_count'] / df['char_count'] # word density\n",
    "\n",
    "# punctuation count\n",
    "def punctuation_count(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "df['punctuation_count'] = df['text'].apply(punctuation_count)\n",
    "\n",
    "# Upper case count\n",
    "def upper_case_count(text):\n",
    "    return sum(1 for char in text if char.isupper())\n",
    "\n",
    "df['upper_case_count'] = df['text'].apply(upper_case_count)\n",
    "\n",
    "def title_word_count(text):\n",
    "    return sum(1 for word in text.split() if word.istitle())\n",
    "\n",
    "df['title_word_count'] = df['text'].apply(title_word_count)\n",
    "\n",
    "# parts of speech\n",
    "def parts_of_speech(text):\n",
    "    pos_tags = pos_tag(word_tokenize(text))\n",
    "    \n",
    "    noun_count = sum(1 for tag in pos_tags if tag[1] in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "    adv_count = sum(1 for tag in pos_tags if tag[1] in ['RB', 'RBR', 'RBS'])\n",
    "    verb_count = sum(1 for tag in pos_tags if tag[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    adj_count = sum(1 for tag in pos_tags if tag[1] in ['JJ', 'JJR', 'JJS'])\n",
    "    pro_count = sum(1 for tag in pos_tags if tag[1] in ['PRP', 'PRP$', 'WP', 'WP$'])\n",
    "    return pd.Series([noun_count, adv_count, verb_count, adj_count, pro_count], index=['noun_count','adv_count','verb_count','adj_count','pro_count'])\n",
    "\n",
    "df[['noun_count','adv_count','verb_count','adj_count','pro_count']] = df['cleaned_text'].apply(lambda x: parts_of_speech(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>pro_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In December 2016 , Google Duo replaced Hangout...</td>\n",
       "      <td>in december google duo replaced hangouts withi...</td>\n",
       "      <td>2295</td>\n",
       "      <td>391</td>\n",
       "      <td>0.17037</td>\n",
       "      <td>111</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>133</td>\n",
       "      <td>14</td>\n",
       "      <td>75</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In December 2016 , Google Duo replaced Hangout...   \n",
       "\n",
       "                                        cleaned_text  char_count  word_count  \\\n",
       "0  in december google duo replaced hangouts withi...        2295         391   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_count  title_word_count  \\\n",
       "0       0.17037                111               101                79   \n",
       "\n",
       "   noun_count  adv_count  verb_count  adj_count  pro_count  \n",
       "0         133         14          75         29          3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [text.split() for text in df['cleaned_text']]\n",
    "\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "\n",
    "corpus_bow = [dictionary.doc2bow(text) for text in corpus]\n",
    "\n",
    "# Training\n",
    "num_topics = 20\n",
    "lda_model = LdaModel(corpus_bow, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "topic_distribution = lda_model.get_document_topics(corpus_bow)\n",
    "\n",
    "for topic in range(num_topics):\n",
    "    df[f'topic_{topic + 1}_score'] = [next((t[1] for t in topic_dist if t[0] == topic), 0) for topic_dist in topic_distribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_11_score</th>\n",
       "      <th>topic_12_score</th>\n",
       "      <th>topic_13_score</th>\n",
       "      <th>topic_14_score</th>\n",
       "      <th>topic_15_score</th>\n",
       "      <th>topic_16_score</th>\n",
       "      <th>topic_17_score</th>\n",
       "      <th>topic_18_score</th>\n",
       "      <th>topic_19_score</th>\n",
       "      <th>topic_20_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, cleaned_text, char_count, word_count, word_density, punctuation_count, upper_case_count, title_word_count, noun_count, adv_count, verb_count, adj_count, pro_count, topic_1_score, topic_2_score, topic_3_score, topic_4_score, topic_5_score, topic_6_score, topic_7_score, topic_8_score, topic_9_score, topic_10_score, topic_11_score, topic_12_score, topic_13_score, topic_14_score, topic_15_score, topic_16_score, topic_17_score, topic_18_score, topic_19_score, topic_20_score]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flesch_kincaid_score'] = df['cleaned_text'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "\n",
    "df['flesch_score'] = df['cleaned_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "\n",
    "df['gunning_fog_score'] = df['cleaned_text'].apply(lambda x: textstat.gunning_fog(x))\n",
    "\n",
    "df['coleman_liau_score'] = df['cleaned_text'].apply(lambda x: textstat.coleman_liau_index(x))\n",
    "\n",
    "df['dale_chall_score'] = df['cleaned_text'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "\n",
    "df['ari_score'] = df['cleaned_text'].apply(lambda x: textstat.automated_readability_index(x))\n",
    "\n",
    "df['linsear_write_score'] = df['cleaned_text'].apply(lambda x: textstat.linsear_write_formula(x))\n",
    "\n",
    "df['spache_score'] = df['cleaned_text'].apply(lambda x: textstat.spache_readability(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_19_score</th>\n",
       "      <th>topic_20_score</th>\n",
       "      <th>flesch_kincaid_score</th>\n",
       "      <th>flesch_score</th>\n",
       "      <th>gunning_fog_score</th>\n",
       "      <th>coleman_liau_score</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>ari_score</th>\n",
       "      <th>linsear_write_score</th>\n",
       "      <th>spache_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, cleaned_text, char_count, word_count, word_density, punctuation_count, upper_case_count, title_word_count, noun_count, adv_count, verb_count, adj_count, pro_count, topic_1_score, topic_2_score, topic_3_score, topic_4_score, topic_5_score, topic_6_score, topic_7_score, topic_8_score, topic_9_score, topic_10_score, topic_11_score, topic_12_score, topic_13_score, topic_14_score, topic_15_score, topic_16_score, topic_17_score, topic_18_score, topic_19_score, topic_20_score, flesch_kincaid_score, flesch_score, gunning_fog_score, coleman_liau_score, dale_chall_score, ari_score, linsear_write_score, spache_score]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    ner_tags = ne_chunk(pos_tags)\n",
    "    ner_count = sum(1 for chunk in ner_tags if hasattr(chunk, 'label'))\n",
    "    return ner_count\n",
    "\n",
    "df['ner_count'] = df['text'].apply(ner_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_20_score</th>\n",
       "      <th>flesch_kincaid_score</th>\n",
       "      <th>flesch_score</th>\n",
       "      <th>gunning_fog_score</th>\n",
       "      <th>coleman_liau_score</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>ari_score</th>\n",
       "      <th>linsear_write_score</th>\n",
       "      <th>spache_score</th>\n",
       "      <th>ner_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In December 2016 , Google Duo replaced Hangout...</td>\n",
       "      <td>in december google duo replaced hangouts withi...</td>\n",
       "      <td>2295</td>\n",
       "      <td>391</td>\n",
       "      <td>0.17037</td>\n",
       "      <td>111</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>133</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>153.4</td>\n",
       "      <td>-308.48</td>\n",
       "      <td>160.29</td>\n",
       "      <td>12.45</td>\n",
       "      <td>27.47</td>\n",
       "      <td>197.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>57.73</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In December 2016 , Google Duo replaced Hangout...   \n",
       "\n",
       "                                        cleaned_text  char_count  word_count  \\\n",
       "0  in december google duo replaced hangouts withi...        2295         391   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_count  title_word_count  \\\n",
       "0       0.17037                111               101                79   \n",
       "\n",
       "   noun_count  adv_count  ...  topic_20_score  flesch_kincaid_score  \\\n",
       "0         133         14  ...               0                 153.4   \n",
       "\n",
       "   flesch_score  gunning_fog_score  coleman_liau_score  dale_chall_score  \\\n",
       "0       -308.48             160.29               12.45             27.47   \n",
       "\n",
       "   ari_score  linsear_write_score  spache_score  ner_count  \n",
       "0      197.0                 62.0         57.73         43  \n",
       "\n",
       "[1 rows x 42 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text error length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = LanguageTool('en-US')\n",
    "\n",
    "def error_length(text):\n",
    "    matches = tool.check(text)\n",
    "    return len(matches)\n",
    "\n",
    "df['error_length'] = df['text'].apply(error_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_score</th>\n",
       "      <th>flesch_score</th>\n",
       "      <th>gunning_fog_score</th>\n",
       "      <th>coleman_liau_score</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>ari_score</th>\n",
       "      <th>linsear_write_score</th>\n",
       "      <th>spache_score</th>\n",
       "      <th>ner_count</th>\n",
       "      <th>error_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In December 2016 , Google Duo replaced Hangout...</td>\n",
       "      <td>in december google duo replaced hangouts withi...</td>\n",
       "      <td>2295</td>\n",
       "      <td>391</td>\n",
       "      <td>0.17037</td>\n",
       "      <td>111</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>133</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>153.4</td>\n",
       "      <td>-308.48</td>\n",
       "      <td>160.29</td>\n",
       "      <td>12.45</td>\n",
       "      <td>27.47</td>\n",
       "      <td>197.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>57.73</td>\n",
       "      <td>43</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In December 2016 , Google Duo replaced Hangout...   \n",
       "\n",
       "                                        cleaned_text  char_count  word_count  \\\n",
       "0  in december google duo replaced hangouts withi...        2295         391   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_count  title_word_count  \\\n",
       "0       0.17037                111               101                79   \n",
       "\n",
       "   noun_count  adv_count  ...  flesch_kincaid_score  flesch_score  \\\n",
       "0         133         14  ...                 153.4       -308.48   \n",
       "\n",
       "   gunning_fog_score  coleman_liau_score  dale_chall_score  ari_score  \\\n",
       "0             160.29               12.45             27.47      197.0   \n",
       "\n",
       "   linsear_write_score  spache_score  ner_count  error_length  \n",
       "0                 62.0         57.73         43            87  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = df\n",
    "\n",
    "# count_vect = df['cleaned_text'].tolist()\n",
    "\n",
    "# count_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# count_matrix = count_vectorizer.fit_transform(count_vect)\n",
    "\n",
    "# count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# df = pd.concat([df, count_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.head(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N - Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bigram Vectorizer\n",
    "\n",
    "# bigram_vect = df['cleaned_text'].tolist()\n",
    "\n",
    "# bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=5000)\n",
    "\n",
    "# bigram_matrix = bigram_vectorizer.fit_transform(bigram_vect)\n",
    "\n",
    "# bigram_df = pd.DataFrame(bigram_matrix.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# df = pd.concat([df, bigram_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trigram Vectorizer\n",
    "\n",
    "# trigram_vect = df['cleaned_text'].tolist()\n",
    "\n",
    "# trigram_vectorizer = CountVectorizer(ngram_range=(3, 3), max_features=5000)\n",
    "\n",
    "# trigram_matrix = trigram_vectorizer.fit_transform(trigram_vect)\n",
    "\n",
    "# trigram_df = pd.DataFrame(trigram_matrix.toarray(), columns=trigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# df = pd.concat([df, trigram_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-Trigram Vectorizer\n",
    "\n",
    "# bitri_vect = df['cleaned_text'].tolist()\n",
    "\n",
    "# bitri_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3), max_features=5000)\n",
    "\n",
    "# bichar_matrix = bitri_vectorizer.fit_transform(bitri_vect)\n",
    "\n",
    "# bichar_df = pd.DataFrame(bichar_matrix.toarray(), columns=bitri_vectorizer.get_feature_names_out())\n",
    "\n",
    "# df = pd.concat([df, bichar_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_score</th>\n",
       "      <th>flesch_score</th>\n",
       "      <th>gunning_fog_score</th>\n",
       "      <th>coleman_liau_score</th>\n",
       "      <th>dale_chall_score</th>\n",
       "      <th>ari_score</th>\n",
       "      <th>linsear_write_score</th>\n",
       "      <th>spache_score</th>\n",
       "      <th>ner_count</th>\n",
       "      <th>error_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In December 2016 , Google Duo replaced Hangout...</td>\n",
       "      <td>in december google duo replaced hangouts withi...</td>\n",
       "      <td>2295</td>\n",
       "      <td>391</td>\n",
       "      <td>0.17037</td>\n",
       "      <td>111</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>133</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>153.4</td>\n",
       "      <td>-308.48</td>\n",
       "      <td>160.29</td>\n",
       "      <td>12.45</td>\n",
       "      <td>27.47</td>\n",
       "      <td>197.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>57.73</td>\n",
       "      <td>43</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In December 2016 , Google Duo replaced Hangout...   \n",
       "\n",
       "                                        cleaned_text  char_count  word_count  \\\n",
       "0  in december google duo replaced hangouts withi...        2295         391   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_count  title_word_count  \\\n",
       "0       0.17037                111               101                79   \n",
       "\n",
       "   noun_count  adv_count  ...  flesch_kincaid_score  flesch_score  \\\n",
       "0         133         14  ...                 153.4       -308.48   \n",
       "\n",
       "   gunning_fog_score  coleman_liau_score  dale_chall_score  ari_score  \\\n",
       "0             160.29               12.45             27.47      197.0   \n",
       "\n",
       "   linsear_write_score  spache_score  ner_count  error_length  \n",
       "0                 62.0         57.73         43            87  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('input.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
